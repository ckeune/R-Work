{
    "contents" : "# analyze survey data for free (http://asdfree.com) with the r language\n# national longitudinal surveys\n# nlsy97, nlsy79, nlsy79cya, older men, mature women, young men, young women\n\n# # # # # # # # # # # # # # # # #\n# # block of code to run this # #\n# # # # # # # # # # # # # # # #\n\n\nrequired_libraries <- c('ggplot2',  \n                        'stringr', \n                        'dplyr',\n                        'scales',\n                        'httr',\n                        'downloader',\n                        'XML',\n                        'stringer')\n\n\nfor (lib_name in required_libraries){\n  if (!require(lib_name, character.only = T)) \n    install.packages(lib_name)\n  library(lib_name, character.only = T)\n} \n\n \nsetwd( \"C:/R/NLS/\" )\nsource_url( \"https://raw.githubusercontent.com/ajdamico/usgsd/master/National%20Longitudinal%20Surveys/download%20all%20microdata.R\" , prompt = FALSE , echo = TRUE )\n# # # # # # # # # # # # # # #\n# # end of auto-run block # #\n# # # # # # # # # # # # # # #\n\n# if you have never used the r language before,\n# watch this two minute video i made outlining\n# how to run this script from start to finish\n# http://www.screenr.com/Zpd8\n\n# anthony joseph damico\n# ajdamico@gmail.com\n\n# if you use this script for a project, please send me a note\n# it's always nice to hear about how people are using this stuff\n\n# for further reading on cross-package comparisons, see:\n# http://journal.r-project.org/archive/2009-2/RJournal_2009-2_Damico.pdf\n\n\n###################################################################################\n# download every file from every available national longitudinal survey study     #\n# with R, saving each extract as an R data frame (.rda) for rapid future analyses #\n###################################################################################\n\n\n# set your working directory.\n# all NLS studies will be stored here after downloading.\n# use forward slashes instead of back slashes\n\n# uncomment this line by removing the `#` at the front..\n# setwd( \"C:/My Directory/NLS/\" )\n# ..in order to set your current working directory\n\n\n\n# remove the # in order to run this install.packages line only once\n# install.packages( c( \"httr\" , \"XML\" , \"stringr\" ) )\n\n\nlibrary(httr)  \t# load httr package (downloads files from the web, with SSL and cookies)\nlibrary(XML)\t\t# load XML (parses through html code to extract links)\nlibrary(stringr)\t# load stringr package (manipulates character strings easily)\nlibrary(downloader)\t# downloads and then runs the source() function on scripts from github\n\n\n# log on to the nlsinfo investigator and pull all available studies\nstudies <- GET( \"https://www.nlsinfo.org/investigator/servlet1?get=STUDIES\" )\n\n# extract the study names option\nstudy.names <- xpathSApply( content( studies ) , \"//option\" , xmlAttrs )\n\n# remove the negative one, which is the default but not its own study\nstudy.names <- study.names[ study.names != '-1' ]\n\n\n# available studies\nprint( studies )\n\n# text names of each study available\nprint( study.names )\n\n\n# this next section is optional\n# but given the pace of the downloads from\n# nlsinfo.org, you might wish to only download\n# a subset of the available cohorts\n# instead of all seven\n\n# # # # # # # # # # # # # # # # # # # # # # # # # # #\n# limit the download to a subset of the nls cohorts #\n# # # # # # # # # # # # # # # # # # # # # # # # # # #\n\n# note: the various cohorts shown on\n# https://www.nlsinfo.org/investigator/pages/login.jsp\n\n# to only download the 1997 cohort, remove the `#` in this next line\n# only.study <- \"NLSY97\"\n\n# to download the 1979 and 1979 child and young adult cohorts, remove the `#` in this next line\n# only.study <- c( \"NLSCYA\" , \"NLSY79\" )\n\n# to download the two original cohorts, remove the `#` in this next line\n# only.study <- c( \"NLSW\" , \"NLSM\" )\n\n\n# no need to edit anything below this line #\n\n# # # # # # # # #\n# program start #\n# # # # # # # # #\n\n# look for an object \"only.study\" and if it exists,\n# limit the studies downloaded to the ones specified\nif( exists( 'only.study' ) ) study.names <- study.names[ study.names %in% only.study ]\n\n\n# initiate a temporary file and temporary directory\ntf <- tempfile() ; td <- tempdir()\n\n\n# loop through all designated studies\nfor ( this.study in study.names ){\n  \n  # determine all available substudies within the selected studies\n  substudies <- \n    GET( \n      paste0( \n        \"https://www.nlsinfo.org/investigator/servlet1?get=SUBSTUDIES&study=\" , \n        this.study \n      ) \n    )\n  \n  # extract the option tags from the substudies page\n  substudy.numbers <- xpathSApply( content( substudies ) , \"//option\" , xmlAttrs )\n  \n  # also extract the values contained within those tags\n  substudy.names <- xpathSApply( content( substudies ) , \"//option\" , xmlValue )\n  \n  # convert that list into a vector\n  substudy.numbers <- unlist( substudy.numbers )\n  \n  # limit the substudies to only actual values, not dropdown list identifiers\n  substudy.numbers <- substudy.numbers[ names( substudy.numbers ) == 'value' ]\n  \n  # just like the overall study names, remove the default negative one selection\n  # from both names..\n  substudy.names <- substudy.names[ substudy.numbers != \"-1\" ]\n  # ..and numbers\n  substudy.numbers <- substudy.numbers[ substudy.numbers != \"-1\" ]\n  \n  # remove leading and trailing spaces from the substudy names\n  substudy.names <- str_trim( substudy.names )\n  \n  # loop through each available substudy\n  for ( study.id in substudy.numbers ){\n    \n    # identify a substudy-specific working directory..\n    this.dir <- paste0( getwd() , \"/\" , substudy.names[ study.id == substudy.numbers ] )\n    \n    # ..and actually create it.  this is where all data extracts will be stored\n    dir.create( this.dir , showWarnings = FALSE , recursive = TRUE )\n    \n    \n    # download the strata and psu for studies where they're available\n    if( study.id == \"1.6\" ){\n      \n      # download the nlsy 1997 cohort's sampling information\n      download( \"https://www.nlsinfo.org/sites/nlsinfo.org/files/attachments/140618/nlsy97stratumpsu.zip\" , tf , mode = 'wb' )\n      \n      # unzip to the local disk\n      z <- unzip( tf , exdir = td )\n      \n      strpsu <- read.csv( z[ grep( '\\\\.csv' , z ) ] )\n      \n      # store the complex sample variables on the local disk\n      save( strpsu , file = paste0( this.dir , \"/strpsu.rda\" ) )\n      \n      # clear up all the bigger objects from working memory\n      rm( strpsu )\n      \n      # clear up RAM\n      gc()\n      \n    }\n    \n    \n    # set the nls investigator to allow downloads for this substudy\n    GET( paste0( \"https://www.nlsinfo.org/investigator/servlet1?set=STUDY&id=\" , study.id ) )\n    \n    # identify all possible extract files for downloading\n    z <- GET( \"https://www.nlsinfo.org/investigator/servlet1?get=SEARCHVALUES&type=RNUM\" )\n    \n    # convert this rnum html result into an xml-readable object\n    doc <- htmlParse( z )\n    \n    # extract all options from this xml text\n    opts <- getNodeSet( doc , \"//select/option\" )\n    \n    # extract all rnum values from within the xml\n    all.option.values <- sapply( opts , xmlGetAttr , \"value\" )\n    \n    # remove any negative ones, since those are not rnums themselves\n    all.option.values <- all.option.values[ all.option.values != \"-1\" ]\n    \n    # loop through each available rnum extract\n    for ( option.value in all.option.values ){\n      \n      # initiate a counter\n      attempt.count <- 0\n      \n      # start off the `attempt` object as an error.\n      attempt <- try( stop() , silent = TRUE )\n      # this is only useful at the start of this next `while` command\n      \n      # so long as the `attempt` object is an error..\n      while( class( attempt ) == 'try-error' ){\n        \n        # add one to the counter\n        attempt.count <- attempt.count + 1\n        \n        # display any actual errors for the user\n        if ( attempt.count > 1 ) print( attempt )\n        \n        # after the fifth attempt, shut down the program.\n        if ( attempt.count > 5 ) stop( \"tried five times with no luck.  peace out.\" )\n        \n        # overwrite the `attempt` object with the result of..\n        attempt <-\n          try( {\n            \n            # re-set the nls investigator to download only the default-included variables\n            GET( paste0( \"https://www.nlsinfo.org/investigator/servlet1?set=STUDY&id=\" , study.id , \"&reset=true\" ) )\n            \n            # print current progress to the screen\n            print( \n              paste( \n                \"currently downloading extract\" , \n                which( option.value == all.option.values ) ,\n                \"of\" ,\n                length( all.option.values ) ,\n                \"extract\" ,\n                option.value , \n                \"attempt\" , \n                attempt.count \n              ) \n            )\n            \n            # initiate the server for downloads\n            GET( \"https://www.nlsinfo.org/investigator/servlet1?set=preference&pref=all\" )\n            \n            # specify the rnum extract to download\n            GET( paste0( \"https://www.nlsinfo.org/investigator/servlet1?get=Results&xml=true&criteria=RNUM%7CSW%7C\" , option.value , \"&sortKey=RNUM&sortOrder=ascending&&PUBID=noid&limit=all\" ) )\n            \n            # rather than downloading only recommended variables, download all of 'em\n            GET( \"https://www.nlsinfo.org/investigator/servlet1?set=tagset&select=all&value=true\" )\n            \n            # specify that only the csv file should be downloaded\n            job.char <- GET( \"https://www.nlsinfo.org/investigator/servlet1?collection=on&sas=off&spss=off&stata=off&codebook=on&csv=on&event=start&cmd=extract&desc=default\" )\n            \n            # extract the specific job id from the above result\n            job.id <- gsub( 'job:' , '' , as.character( job.char ) )\n            \n            # trigger the creation of the extract\n            GET( \"https://www.nlsinfo.org/investigator/servlet1?get=downloads&study=current\" )\n            \n            # start out with a blank string\n            v <- \"\"\n            \n            # so long as the `v` string does not contain this response text..\n            while( !( grepl( \"{\\\"status_response\\\":{\\\"message\\\":\\\"\\\",\\\"name\\\"\" , as.character( v ) , fixed = TRUE ) ) ){\n              \n              # ping the server to determine the current progress of the creation of the current extract\n              v <- GET( paste0( \"https://www.nlsinfo.org/investigator/servlet1?job=\" , job.id , \"&event=progress&cmd=extract&_=\" , as.numeric( Sys.time() ) * 1000 ) )\n              \n              # if the download hits an error, break out of the current loop.\n              ep <- FALSE\n              \n              # see if the current page contains an error page text, instead of actual data.\n              try( ep <- xpathSApply( htmlParse( v , asText = TRUE ) , '//title' , xmlValue ) == 'Error Page' , silent = TRUE )\n              \n              # if it does contain an error page, break the program inside this current try loop\n              if( ( length( ep ) > 0 ) && ( ep ) ) stop( \"Error Page\" )\n              # first successful usage of `&&` operator.  pat on the back.\n              \n              # extract the current contents of the `v` object to determine the current progress\n              msg <- strsplit( strsplit( as.character(v) , 'message\\\":\\\"' )[[1]][2] , '\\\",\\\"name' )[[1]][1]\n              \n              # print that progress to the screen\n              cat( \"    \" , msg , \"\\r\" )\n              \n              # give the progress bar fifteen seconds before it\n              # refreshes so it's not overloading the website\n              Sys.sleep( 15 )\n              \n            }\n            # once the extract creation has been completed\n            \n            # initiate an empty `u` object..\n            u <- NULL\n            \n            # ..with a failed-error code\n            u$headers$status <- 500\n            \n            # initiate a timer\n            start.time <- Sys.time()\n            \n            # so long as the status code returns unfinished\n            while( !is.null( u$headers$status ) && u$headers$status == 500 ){\n              \n              # if you've been waiting more than two minutes, just stop.\n              if ( Sys.time() - start.time > 120 ) stop( 'waited two minutes after extract created, still no download' )\n              \n              # download the zipped file for this specific job id\n              u <- GET( paste0( \"https://www.nlsinfo.org/investigator/downloads/\" , job.id , \"/default.zip\" ) )\n              \n            }\n            \n            # save that result zipped file into the temporary file on your local disk\n            writeBin( content( u , \"raw\" ) , tf )\n            \n            # unzip that temporary file into the temporary directory on your local disk\n            d <- unzip( tf , exdir = td )\n            \n            # determine the location of the `.csv` file within the zipped file you've just unarchived\n            csv <- d[ grep( '.csv' , d , fixed = TRUE ) ]\n            \n            # save that zipped file as a data.frame named as the current `option.value`\n            assign( option.value , read.csv( csv ) )\n            \n            # store that `option.value` in the current save-location\n            save( list = option.value , file = paste0( this.dir , \"/\" , option.value , \".rda\" ) )\n            \n            # clear up all the bigger objects from working memory\n            rm( list = c( 'u' , option.value ) )\n            \n            # clear up RAM\n            gc()\n            \n          } , \n          silent = TRUE \n          )\n        \n        # wait the same number of minutes as you have attempted-counted,\n        # but after the last attempt, don't wait at all.\n        if( class( attempt ) == 'try-error' ) Sys.sleep( 60 * ifelse( attempt.count >= 5 , 0 , attempt.count ) ) else Sys.sleep( 15 )\n        \n      }\n      \n    }\n    \n  }\n}\n\n# remove the temporary file..\nfile.remove( tf )\n\n# ..and temporary directory from your hard drive\nunlink( d , recursive = TRUE )\n\n\n# for more details on how to work with data in r\n# check out my two minute tutorial video site\n# http://www.twotorials.com/\n\n# dear everyone: please contribute your script.\n# have you written syntax that precisely matches an official publication?\nmessage( \"if others might benefit, send your code to ajdamico@gmail.com\" )\n# http://asdfree.com needs more user contributions\n\n# let's play the which one of these things doesn't belong game:\n# \"only you can prevent forest fires\" -smokey bear\n# \"take a bite out of crime\" -mcgruff the crime pooch\n# \"plz gimme your statistical programming\" -anthony damico",
    "created" : 1420818288433.000,
    "dirty" : true,
    "encoding" : "",
    "folds" : "",
    "hash" : "2284876177",
    "id" : "BC8E4949",
    "lastKnownWriteTime" : 280332992,
    "path" : null,
    "project_path" : null,
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_source"
}